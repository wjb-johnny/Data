{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "748e245881e9a26d54da4c9431786b3f874734de"
   },
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pandas and numpy for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# featuretools for automated feature engineering\n",
    "import featuretools as ft\n",
    "\n",
    "# matplotlit and seaborn for visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.size'] = 22\n",
    "import seaborn as sns\n",
    "\n",
    "# Suppress warnings from pandas\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# modeling \n",
    "import lightgbm as lgb\n",
    "\n",
    "# utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# memory management\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in data\n",
    "train_bureau = pd.read_csv('../input/home-credit-manual-engineered-features/train_bureau_raw.csv', nrows = 1000)\n",
    "test_bureau = pd.read_csv('../input/home-credit-manual-engineered-features/test_bureau_raw.csv', nrows = 1000)\n",
    "\n",
    "train_previous = pd.read_csv('../input/home-credit-manual-engineered-features/train_previous_raw.csv', nrows = 1000)\n",
    "test_previous = pd.read_csv('../input/home-credit-manual-engineered-features/test_previous_raw.csv', nrows = 1000)\n",
    "\n",
    "# All columns in dataframes\n",
    "bureau_columns = list(train_bureau.columns)\n",
    "previous_columns = list(train_previous.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "af78939941b510855d5a972ca24e33037c003977"
   },
   "outputs": [],
   "source": [
    "# Bureau only features\n",
    "bureau_features = list(set(bureau_columns) - set(previous_columns))\n",
    "\n",
    "# Previous only features\n",
    "previous_features = list(set(previous_columns) - set(bureau_columns))\n",
    "\n",
    "# Original features will be in both datasets\n",
    "original_features = list(set(previous_columns) & set(bureau_columns))\n",
    "\n",
    "print('There are %d original features.' % len(original_features))\n",
    "print('There are %d bureau and bureau balance features.' % len(bureau_features))\n",
    "print('There are %d previous Home Credit loan features.' % len(previous_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "b0da702050879ddfb8aecc4c23186e7ee97923ba"
   },
   "outputs": [],
   "source": [
    "train_labels = train_bureau['TARGET']\n",
    "previous_features.append('SK_ID_CURR')\n",
    "\n",
    "train_ids = train_bureau['SK_ID_CURR']\n",
    "test_ids = test_bureau['SK_ID_CURR']\n",
    "\n",
    "# Merge the dataframes avoiding duplicating columns by subsetting train_previous\n",
    "train = train_bureau.merge(train_previous[previous_features], on = 'SK_ID_CURR')\n",
    "test = test_bureau.merge(test_previous[previous_features], on = 'SK_ID_CURR')\n",
    "\n",
    "print('Training shape: ', train.shape)\n",
    "print('Testing shape: ', test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "356f67ea6d19d408bc19e1a58ee5f23693a3944a"
   },
   "outputs": [],
   "source": [
    "# One hot encoding\n",
    "train = pd.get_dummies(train)\n",
    "test = pd.get_dummies(test)\n",
    "\n",
    "# Match the columns in the dataframes\n",
    "train, test = train.align(test, join = 'inner', axis = 1)\n",
    "print('Training shape: ', train.shape)\n",
    "print('Testing shape: ', test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ecd8a30162d43fe05e14ef93cb88de29f933f8e9"
   },
   "source": [
    "### Correct Mistakes\n",
    "remove any columns built on the `SK_ID_CURR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "330d43c9cf3e7236fbd4a6852a10efd014b433ce"
   },
   "outputs": [],
   "source": [
    "cols_with_id = [x for x in train.columns if 'SK_ID_CURR' in x]\n",
    "cols_with_bureau_id = [x for x in train.columns if 'SK_ID_BUREAU' in x]\n",
    "cols_with_previous_id = [x for x in train.columns if 'SK_ID_PREV' in x]\n",
    "print('There are %d columns that contain SK_ID_CURR' % len(cols_with_id))\n",
    "print('There are %d columns that contain SK_ID_BUREAU' % len(cols_with_bureau_id))\n",
    "print('There are %d columns that contain SK_ID_PREV' % len(cols_with_previous_id))\n",
    "\n",
    "train = train.drop(columns = cols_with_id)\n",
    "test = test.drop(columns = cols_with_id)\n",
    "print('Training shape: ', train.shape)\n",
    "print('Testing shape: ', test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c402926c8d265ad46e8f9bc5b1683563352cb496"
   },
   "source": [
    "# Remove Collinear Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5100a4ad6c6586b2637139db8ef4e67df5ad49a8"
   },
   "source": [
    "### Identify Correlated Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "59259cf2be69f0c5e699109f9177287599836354"
   },
   "outputs": [],
   "source": [
    "# Threshold for removing correlated variables\n",
    "threshold = 0.9\n",
    "\n",
    "# Absolute value correlation matrix\n",
    "corr_matrix = train.corr().abs()\n",
    "corr_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "7c33392948540b626cf213fa6dea316766627eb9"
   },
   "outputs": [],
   "source": [
    "# Upper triangle of correlations\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "upper.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "1517d554f80a8eed2c0e03303fe7d1fb4a469131"
   },
   "outputs": [],
   "source": [
    "# Select columns with correlations above threshold\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "\n",
    "print('There are %d columns to remove.' % (len(to_drop)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fc6bf1f1a4c98749727522bb5a3ae89adf145f8d"
   },
   "source": [
    "#### Drop Correlated Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "2c18834968dac305317a5a881308a0c9a3cce353"
   },
   "outputs": [],
   "source": [
    "train = train.drop(columns = to_drop)\n",
    "test = test.drop(columns = to_drop)\n",
    "\n",
    "print('Training shape: ', train.shape)\n",
    "print('Testing shape: ', test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "52eada9c579b09cc97c338ea372554aa8d52f9df"
   },
   "source": [
    "Applying this on the entire dataset __results in 538  collinear features__ removed.  \n",
    "\n",
    "The full datasets: `m_train_combined.csv` and `m_test_combined.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "15973fdfb3169e1d816136c68764c80d1368000e"
   },
   "source": [
    "### Read in Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e8384e3033ee466ab3c426d8330822e974ad3c1b"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/home-credit-manual-engineered-features/m_train_combined.csv')\n",
    "test = pd.read_csv('../input/home-credit-manual-engineered-features/m_test_combined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "18d3be4bd305a82d6bba1c2a6b7d2876abb098d9",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Training set full shape: ', train.shape)\n",
    "print('Testing set full shape: ' , test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e9c652ca3eb3a8770195469abf77ad71d15b3fe0"
   },
   "source": [
    "# Remove Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_uuid": "7849b6e3f53cd08850e9e5797e7256b08a823acb",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train missing values (in percent)\n",
    "train_missing = (train.isnull().sum() / len(train)).sort_values(ascending = False)\n",
    "train_missing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_uuid": "f7b485689bedd322ea3eb9083822f814719add0b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test missing values (in percent)\n",
    "test_missing = (test.isnull().sum() / len(test)).sort_values(ascending = False)\n",
    "test_missing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_uuid": "9ab5d9001ab54692f0e2bc4e1d976d734771891c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Identify missing values above threshold\n",
    "train_missing = train_missing.index[train_missing > 0.75]\n",
    "test_missing = test_missing.index[test_missing > 0.75]\n",
    "\n",
    "all_missing = list(set(set(train_missing) | set(test_missing)))\n",
    "print('There are %d columns with more than 75%% missing values' % len(all_missing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fc8e7bdb14e7e65a257c93d5c45a882098971360"
   },
   "source": [
    "drop the columns, one-hot encode the dataframes, and then align the columns of the dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_uuid": "9478bfc3069bf9b4e192c45cf323c1ea84ca7b2c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Need to save the labels because aligning will remove this column\n",
    "train_labels = train[\"TARGET\"]\n",
    "train_ids = train['SK_ID_CURR']\n",
    "test_ids = test['SK_ID_CURR']\n",
    "\n",
    "train = pd.get_dummies(train.drop(columns = all_missing))\n",
    "test = pd.get_dummies(test.drop(columns = all_missing))\n",
    "\n",
    "train, test = train.align(test, join = 'inner', axis = 1)\n",
    "\n",
    "print('Training set full shape: ', train.shape)\n",
    "print('Testing set full shape: ' , test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_uuid": "06a7110748fe39b0ae8248ed62dfeb844a124ca1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train.drop(columns = ['SK_ID_CURR'])\n",
    "test = test.drop(columns = ['SK_ID_CURR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d4d38ab010cddd88e714894e09d8733cc60daafd"
   },
   "source": [
    "# Feature Selection through Feature Importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6525f690933a50ab1eb7f360629b98d9a2f6df37"
   },
   "source": [
    "Since the LightGBM model does not need missing values to be imputed, we can directly `fit` on the training data. We will use Early Stopping to determine the optimal number of iterations and run the model twice, averaging the feature importances to try and avoid overfitting to a certain set of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "_uuid": "4741055961c566f3a2e9d60893950cade5a57330",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize an empty array to hold feature importances\n",
    "feature_importances = np.zeros(train.shape[1])\n",
    "\n",
    "# Create the model with several hyperparameters\n",
    "model = lgb.LGBMClassifier(objective='binary', boosting_type = 'goss', n_estimators = 10000, class_weight = 'balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "_uuid": "4535a84ea5b2c55f7b443832c7a5b2b77894e511",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit the model twice to avoid overfitting\n",
    "for i in range(2):\n",
    "    \n",
    "    # Split into training and validation set\n",
    "    train_features, valid_features, train_y, valid_y = train_test_split(train, train_labels, test_size = 0.25, random_state = i)\n",
    "    \n",
    "    # Train using early stopping\n",
    "    model.fit(train_features, train_y, early_stopping_rounds=100, eval_set = [(valid_features, valid_y)], \n",
    "              eval_metric = 'auc', verbose = 200)\n",
    "    \n",
    "    # Record the feature importances\n",
    "    feature_importances += model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "_uuid": "68a0ce264d8182bffa27fca668d81040fdc3b9ba",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make sure to average feature importances! \n",
    "feature_importances = feature_importances / 2\n",
    "feature_importances = pd.DataFrame({'feature': list(train.columns), 'importance': feature_importances}).sort_values('importance', ascending = False)\n",
    "\n",
    "feature_importances.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "_uuid": "54f05dafeccd5f3800967fa304cbcf3b3076a5c2",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find the features with zero importance\n",
    "zero_features = list(feature_importances[feature_importances['importance'] == 0.0]['feature'])\n",
    "print('There are %d features with 0.0 importance' % len(zero_features))\n",
    "feature_importances.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "68f738649fcc774a3eeceb2c602d00d9f4ff841d"
   },
   "source": [
    "We see that one of our features made it into the top 5 most important! That's a good sign for all of our hard work making the features. It also looks like many of the features we made have literally 0 importance. For the gradient boosting machine, features with 0 importance are not used at all to make any splits. Therefore, we can remove these features from the model with no effect on performance (except for faster training). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "_uuid": "144732dd9d88d13b06983590b9d34970ff6c9a69",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_feature_importances(df, threshold = 0.9):\n",
    "    \"\"\"\n",
    "    Plots 15 most important features and the cumulative importance of features.\n",
    "    Prints the number of features needed to reach threshold cumulative importance.\n",
    "    \n",
    "    Parameters\n",
    "    --------\n",
    "    df : dataframe\n",
    "        Dataframe of feature importances. Columns must be feature and importance\n",
    "    threshold : float, default = 0.9\n",
    "        Threshold for prining information about cumulative importances\n",
    "        \n",
    "    Return\n",
    "    --------\n",
    "    df : dataframe\n",
    "        Dataframe ordered by feature importances with a normalized column (sums to 1)\n",
    "        and a cumulative importance column\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    plt.rcParams['font.size'] = 18\n",
    "    \n",
    "    # Sort features according to importance\n",
    "    df = df.sort_values('importance', ascending = False).reset_index()\n",
    "    \n",
    "    # Normalize the feature importances to add up to one\n",
    "    df['importance_normalized'] = df['importance'] / df['importance'].sum()\n",
    "    df['cumulative_importance'] = np.cumsum(df['importance_normalized'])\n",
    "\n",
    "    # Make a horizontal bar chart of feature importances\n",
    "    plt.figure(figsize = (10, 6))\n",
    "    ax = plt.subplot()\n",
    "    \n",
    "    # Need to reverse the index to plot most important on top\n",
    "    ax.barh(list(reversed(list(df.index[:15]))), \n",
    "            df['importance_normalized'].head(15), \n",
    "            align = 'center', edgecolor = 'k')\n",
    "    \n",
    "    # Set the yticks and labels\n",
    "    ax.set_yticks(list(reversed(list(df.index[:15]))))\n",
    "    ax.set_yticklabels(df['feature'].head(15))\n",
    "    \n",
    "    # Plot labeling\n",
    "    plt.xlabel('Normalized Importance'); plt.title('Feature Importances')\n",
    "    plt.show()\n",
    "    \n",
    "    # Cumulative importance plot\n",
    "    plt.figure(figsize = (8, 6))\n",
    "    plt.plot(list(range(len(df))), df['cumulative_importance'], 'r-')\n",
    "    plt.xlabel('Number of Features'); plt.ylabel('Cumulative Importance'); \n",
    "    plt.title('Cumulative Feature Importance');\n",
    "    plt.show();\n",
    "    \n",
    "    importance_index = np.min(np.where(df['cumulative_importance'] > threshold))\n",
    "    print('%d features required for %0.2f of cumulative importance' % (importance_index + 1, threshold))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "_uuid": "ea0dcbff339b6ffdfd654d165cdb96268311ba95",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "norm_feature_importances = plot_feature_importances(feature_importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e8df33430fe5bd31d83e5d2ca5050c0864b34488"
   },
   "source": [
    "remove the features that have zero importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "_uuid": "e3849b51615d52eee53e3c919c17c6066ae29397",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train.drop(columns = zero_features)\n",
    "test = test.drop(columns = zero_features)\n",
    "\n",
    "print('Training shape: ', train.shape)\n",
    "print('Testing shape: ', test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c67926fdcb81d3f438510f06cadbba1edbc70879"
   },
   "source": [
    "At this point, we can re-run the model to see if it identifies any more features with zero importance. In a way, we are implementing our own form of recursive feature elimination. Since we are repeating work, we should probably put the zero feature importance identification code in a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "_uuid": "d09ec2ad863688243bc95c51aa43a559f56ad4fb",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def identify_zero_importance_features(train, train_labels, iterations = 2):\n",
    "    \"\"\"\n",
    "    Identify zero importance features in a training dataset based on the \n",
    "    feature importances from a gradient boosting model. \n",
    "    \n",
    "    Parameters\n",
    "    --------\n",
    "    train : dataframe\n",
    "        Training features\n",
    "        \n",
    "    train_labels : np.array\n",
    "        Labels for training data\n",
    "        \n",
    "    iterations : integer, default = 2\n",
    "        Number of cross validation splits to use for determining feature importances\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize an empty array to hold feature importances\n",
    "    feature_importances = np.zeros(train.shape[1])\n",
    "\n",
    "    # Create the model with several hyperparameters\n",
    "    model = lgb.LGBMClassifier(objective='binary', boosting_type = 'goss', n_estimators = 10000, class_weight = 'balanced')\n",
    "    \n",
    "    # Fit the model multiple times to avoid overfitting\n",
    "    for i in range(iterations):\n",
    "\n",
    "        # Split into training and validation set\n",
    "        train_features, valid_features, train_y, valid_y = train_test_split(train, train_labels, test_size = 0.25, random_state = i)\n",
    "\n",
    "        # Train using early stopping\n",
    "        model.fit(train_features, train_y, early_stopping_rounds=100, eval_set = [(valid_features, valid_y)], \n",
    "                  eval_metric = 'auc', verbose = 200)\n",
    "\n",
    "        # Record the feature importances\n",
    "        feature_importances += model.feature_importances_ / iterations\n",
    "    \n",
    "    feature_importances = pd.DataFrame({'feature': list(train.columns), 'importance': feature_importances}).sort_values('importance', ascending = False)\n",
    "    \n",
    "    # Find the features with zero importance\n",
    "    zero_features = list(feature_importances[feature_importances['importance'] == 0.0]['feature'])\n",
    "    print('\\nThere are %d features with 0.0 importance' % len(zero_features))\n",
    "    \n",
    "    return zero_features, feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "_uuid": "f69eede0e8e04f1f4c850e5c102dcdebcdb107e7",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "second_round_zero_features, feature_importances = identify_zero_importance_features(train, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c877ed509374d9494c508a8b920e92c926453f84"
   },
   "source": [
    "There are now no 0 importance features left (I guess we should have expected this). If we want to remove more features, we will have to start with features that have a non-zero importance. One way we could do this is by retaining enough features to account for a threshold percentage of importance, such as 95%. At this point, let's keep enough features to account for 95% of the importance. Again, this is an arbitrary decision! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "_uuid": "bb6dd17a7417f3b67017e7e8e18c09725df6e434",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "norm_feature_importances = plot_feature_importances(feature_importances, threshold = 0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1cfc6829b62a44efcc7aeba227db4b293b76bfd5"
   },
   "source": [
    "We can keep only the features needed for 95% importance. This step seems to me to have the greatest chance of harming the model's learning ability, so rather than changing the original dataset, we will make smaller copies. Then, we can test both versions of the data to see if the extra feature removal step is worthwhile. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "_uuid": "55a534e48b21d4038413a11ff3aad5e025bba5bd",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Threshold for cumulative importance\n",
    "threshold = 0.95\n",
    "\n",
    "# Extract the features to keep\n",
    "features_to_keep = list(norm_feature_importances[norm_feature_importances['cumulative_importance'] < threshold]['feature'])\n",
    "\n",
    "# Create new datasets with smaller features\n",
    "train_small = train[features_to_keep]\n",
    "test_small = test[features_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "_uuid": "b004fb636fee6ad313296c185273a5c0058afc78",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_small['TARGET'] = train_labels\n",
    "train_small['SK_ID_CURR'] = train_ids\n",
    "test_small['SK_ID_CURR'] = test_ids\n",
    "\n",
    "train_small.to_csv('m_train_small.csv', index = False)\n",
    "test_small.to_csv('m_test_small.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b4f2c70170ca1ec372e9646edecac79bba561cd6"
   },
   "source": [
    "# Test New Featuresets\n",
    "standard LightGBM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "_uuid": "cf8d66553230c1a535438f3a453ca3a05da7bc29",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(features, test_features, encoding = 'ohe', n_folds = 5):\n",
    "    \n",
    "    \"\"\"Train and test a light gradient boosting model using\n",
    "    cross validation. \n",
    "    \n",
    "    Parameters\n",
    "    --------\n",
    "        features (pd.DataFrame): \n",
    "            dataframe of training features to use \n",
    "            for training a model. Must include the TARGET column.\n",
    "        test_features (pd.DataFrame): \n",
    "            dataframe of testing features to use\n",
    "            for making predictions with the model. \n",
    "        encoding (str, default = 'ohe'): \n",
    "            method for encoding categorical variables. Either 'ohe' for one-hot encoding or 'le' for integer label encoding\n",
    "            n_folds (int, default = 5): number of folds to use for cross validation\n",
    "        \n",
    "    Return\n",
    "    --------\n",
    "        submission (pd.DataFrame): \n",
    "            dataframe with `SK_ID_CURR` and `TARGET` probabilities\n",
    "            predicted by the model.\n",
    "        feature_importances (pd.DataFrame): \n",
    "            dataframe with the feature importances from the model.\n",
    "        valid_metrics (pd.DataFrame): \n",
    "            dataframe with training and validation metrics (ROC AUC) for each fold and overall.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract the ids\n",
    "    train_ids = features['SK_ID_CURR']\n",
    "    test_ids = test_features['SK_ID_CURR']\n",
    "    \n",
    "    # Extract the labels for training\n",
    "    labels = features['TARGET']\n",
    "    \n",
    "    # Remove the ids and target\n",
    "    features = features.drop(columns = ['SK_ID_CURR', 'TARGET'])\n",
    "    test_features = test_features.drop(columns = ['SK_ID_CURR'])\n",
    "    \n",
    "    \n",
    "    # One Hot Encoding\n",
    "    if encoding == 'ohe':\n",
    "        features = pd.get_dummies(features)\n",
    "        test_features = pd.get_dummies(test_features)\n",
    "        \n",
    "        # Align the dataframes by the columns\n",
    "        features, test_features = features.align(test_features, join = 'inner', axis = 1)\n",
    "        \n",
    "        # No categorical indices to record\n",
    "        cat_indices = 'auto'\n",
    "    \n",
    "    # Integer label encoding\n",
    "    elif encoding == 'le':\n",
    "        \n",
    "        # Create a label encoder\n",
    "        label_encoder = LabelEncoder()\n",
    "        \n",
    "        # List for storing categorical indices\n",
    "        cat_indices = []\n",
    "        \n",
    "        # Iterate through each column\n",
    "        for i, col in enumerate(features):\n",
    "            if features[col].dtype == 'object':\n",
    "                # Map the categorical features to integers\n",
    "                features[col] = label_encoder.fit_transform(np.array(features[col].astype(str)).reshape((-1,)))\n",
    "                test_features[col] = label_encoder.transform(np.array(test_features[col].astype(str)).reshape((-1,)))\n",
    "\n",
    "                # Record the categorical indices\n",
    "                cat_indices.append(i)\n",
    "    \n",
    "    # Catch error if label encoding scheme is not valid\n",
    "    else:\n",
    "        raise ValueError(\"Encoding must be either 'ohe' or 'le'\")\n",
    "        \n",
    "    print('Training Data Shape: ', features.shape)\n",
    "    print('Testing Data Shape: ', test_features.shape)\n",
    "    \n",
    "    # Extract feature names\n",
    "    feature_names = list(features.columns)\n",
    "    \n",
    "    # Convert to np arrays\n",
    "    features = np.array(features)\n",
    "    test_features = np.array(test_features)\n",
    "    \n",
    "    # Create the kfold object\n",
    "    k_fold = KFold(n_splits = n_folds, shuffle = False, random_state = 50)\n",
    "    \n",
    "    # Empty array for feature importances\n",
    "    feature_importance_values = np.zeros(len(feature_names))\n",
    "    \n",
    "    # Empty array for test predictions\n",
    "    test_predictions = np.zeros(test_features.shape[0])\n",
    "    \n",
    "    # Empty array for out of fold validation predictions\n",
    "    out_of_fold = np.zeros(features.shape[0])\n",
    "    \n",
    "    # Lists for recording validation and training scores\n",
    "    valid_scores = []\n",
    "    train_scores = []\n",
    "    \n",
    "    # Iterate through each fold\n",
    "    for train_indices, valid_indices in k_fold.split(features):\n",
    "        \n",
    "        # Training data for the fold\n",
    "        train_features, train_labels = features[train_indices], labels[train_indices]\n",
    "        # Validation data for the fold\n",
    "        valid_features, valid_labels = features[valid_indices], labels[valid_indices]\n",
    "        \n",
    "        # Create the model\n",
    "        model = lgb.LGBMClassifier(n_estimators=10000, objective = 'binary', boosting_type='goss',\n",
    "                                   class_weight = 'balanced', learning_rate = 0.05, \n",
    "                                   reg_alpha = 0.1, reg_lambda = 0.1, n_jobs = -1, random_state = 50)\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(train_features, train_labels, eval_metric = 'auc',\n",
    "                  eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n",
    "                  eval_names = ['valid', 'train'], categorical_feature = cat_indices,\n",
    "                  early_stopping_rounds = 100, verbose = 200)\n",
    "        \n",
    "        # Record the best iteration\n",
    "        best_iteration = model.best_iteration_\n",
    "        \n",
    "        # Record the feature importances\n",
    "        feature_importance_values += model.feature_importances_ / k_fold.n_splits\n",
    "        \n",
    "        # Make predictions\n",
    "        test_predictions += model.predict_proba(test_features, num_iteration = best_iteration)[:, 1] / k_fold.n_splits\n",
    "        \n",
    "        # Record the out of fold predictions\n",
    "        out_of_fold[valid_indices] = model.predict_proba(valid_features, num_iteration = best_iteration)[:, 1]\n",
    "        \n",
    "        # Record the best score\n",
    "        valid_score = model.best_score_['valid']['auc']\n",
    "        train_score = model.best_score_['train']['auc']\n",
    "        \n",
    "        valid_scores.append(valid_score)\n",
    "        train_scores.append(train_score)\n",
    "        \n",
    "        # Clean up memory\n",
    "        gc.enable()\n",
    "        del model, train_features, valid_features\n",
    "        gc.collect()\n",
    "        \n",
    "    # Make the submission dataframe\n",
    "    submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': test_predictions})\n",
    "    \n",
    "    # Make the feature importance dataframe\n",
    "    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n",
    "    \n",
    "    # Overall validation score\n",
    "    valid_auc = roc_auc_score(labels, out_of_fold)\n",
    "    \n",
    "    # Add the overall scores to the metrics\n",
    "    valid_scores.append(valid_auc)\n",
    "    train_scores.append(np.mean(train_scores))\n",
    "    \n",
    "    # Needed for creating dataframe of validation scores\n",
    "    fold_names = list(range(n_folds))\n",
    "    fold_names.append('overall')\n",
    "    \n",
    "    # Dataframe of validation scores\n",
    "    metrics = pd.DataFrame({'fold': fold_names,\n",
    "                            'train': train_scores,\n",
    "                            'valid': valid_scores}) \n",
    "    \n",
    "    return submission, feature_importances, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e116f579927c9dbb930ca2cf313e1a42fe0e61c5"
   },
   "source": [
    "### Test \"Full\" Dataset\n",
    "\n",
    "This is the expanded dataset. To recap the process to make this dataset we:\n",
    "\n",
    "* Removed collinear features as measured by the correlation coefficient greater than 0.9\n",
    "* Removed any columns with greater than 80% missing values in the train or test set\n",
    "* Removed all features with non-zero feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "_uuid": "3560addd3c31f5c21235062257bf022e31a872d3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['TARGET'] = train_labels\n",
    "train['SK_ID_CURR'] = train_ids\n",
    "test['SK_ID_CURR'] = test_ids\n",
    "\n",
    "submission, feature_importances, metrics = model(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "_uuid": "400cf0845130330e64dd59705a16488d077f4d27",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "_uuid": "bd41dd2fc42440ce495fd42e4819b00e5658b656",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission.to_csv('selected_features_submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fe3efd3457d02bf99dfa033d01f870487f3314c1"
   },
   "source": [
    "The full features after feature selection score __0.783__ when submitted to the public leaderboard. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b5e46e877e2be014cb9a821de7c93efb6d90f036"
   },
   "source": [
    "### Test \"Small\" Dataset\n",
    "\n",
    "The small dataset requires one additional step over the ful l dataset:\n",
    "\n",
    "* Keep only features needed to reach 95% cumulative importance in the gradient boosting machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "_uuid": "a3a34bde5e4be879d334fff74a246c6c11b679e6",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission_small, feature_importances_small, metrics_small = model(train_small, test_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "_uuid": "beed5d5aed49e7a61db907778b8b717729697d76",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metrics_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "_uuid": "edb183a37eaeff22deb3f5afb04b6da480997e63",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission_small.to_csv('selected_features_small_submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "510e06476d8106c2fda7eb857ff55e5caf57ccfe"
   },
   "source": [
    "The smaller featureset scores __0.782__ when submitted to the public leaderboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "054116a7cd95f8baeecf4ada05a21eddaaaad89e"
   },
   "source": [
    "# Conclusions\n",
    "\n",
    "In this notebook we employed a number of feature selection methods. These methods are necessary to reduce the number of features to increase model interpretability, decrease model runtime, and increase generalization performance on the test set. The methods of feature selection we used are:\n",
    "\n",
    "1. Remove highly collinear variables as measured by a correlation coefficient greater than 0.9\n",
    "2. Remove any columns with more than 75% missing values.\n",
    "3. Remove any features with a zero importance as determined by a gradient boosting machine.\n",
    "4. (Optional) keep only enough features to account for 95% of the importance in the gradient boosting machine.\n",
    "\n",
    "Using the first three methods, we reduced the number of features from __1465__ to __536__ with a 5-fold cv AUC ROC score of 0.7838 and a public leaderboard score of 0.783.\n",
    "\n",
    "After applying the fourth method, we end up with 342 features with a 5-fold cv AUC SCORE of 0.7482 and a public leaderboard score of 0.782. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "07501aa5454fd60c183efb98c733b953ba6ae10d",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
